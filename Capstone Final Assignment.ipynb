{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries needed\n",
    "# !pip install geopandas\n",
    "# !pip install geopy    # Done @25/4\n",
    "# !conda install -c conda-forge --no-deps folium=0.10.0 --yes\n",
    "import geopy\n",
    "from geopy.geocoders import Nominatim\n",
    "from collections import OrderedDict\n",
    "import json, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n",
    "from geopy.geocoders import Nominatim\n",
    "# !pip install folium   # Done @25/4\n",
    "import folium \n",
    "# Matplotlib and associated plotting modules\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib inline\n",
    "# !pip install matplotlib_venn   # Done @25/4\n",
    "from matplotlib_venn import venn2\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New York data to read\n",
    "url = 'https://cocl.us/new_york_dataset'\n",
    "ny_data = requests.get(url).json()\n",
    "\n",
    "# relevant information is in 'features' key\n",
    "ny_data = ny_data['features']\n",
    "ny_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the information from the json file into a dataframe\n",
    "tmp_columns = ['Borough', 'Neighbourhood', 'Latitude', 'Longitude']\n",
    "ny_df = pd.DataFrame(columns = tmp_columns)\n",
    "for data in ny_data:\n",
    "    borough = data['properties']['borough']\n",
    "    neigh   = data['properties']['name']\n",
    "    lat_lon = data['geometry']['coordinates'] # now it'll return list\n",
    "    lon, lat = lat_lon[0], lat_lon[1]\n",
    "    \n",
    "    ny_df = ny_df.append({'Borough': borough,'Neighbourhood': neigh, 'Latitude': lat,\n",
    "                          'Longitude': lon}, ignore_index=True)\n",
    "    \n",
    "ny_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toronto data - Scraping and Cleaning\n",
    "\n",
    "# Wikipedia url for our Toronto neighborhoods data; an earlier version for the correct data structure\n",
    "url = 'https://en.wikipedia.org/w/index.php?title=List_of_postal_codes_of_Canada:_M&oldid=942851379'\n",
    "# Beautifulsoup to download html data\n",
    "req = requests.get(url)\n",
    "soup = BeautifulSoup(req.content,'lxml')\n",
    "table = soup.find_all('table')[0]\n",
    "df = pd.read_html(str(table))\n",
    "neighborhood=pd.DataFrame(df[0])\n",
    "\n",
    "# Drop \"Not Assigned\" Neighborhoods\n",
    "neighborhood['Neighbourhood'].replace('Not assigned', np.nan, inplace=True)\n",
    "neighborhood.dropna(subset=['Neighbourhood'], inplace=True)\n",
    "neighborhood.reset_index(drop=True, inplace=True)\n",
    "# Check our current data\n",
    "print('Our dataframe has {} rows in total and {} with value \"Not Assigned\" in the Neighbourhood column'.\n",
    "      format(neighborhood.shape[0], len(neighborhood[neighborhood['Neighbourhood']=='Not assigned'])))\n",
    "# Check if there are \"Boroughs\" with \"NA\" values, in order to replace them with their respective Neighborhood\n",
    "print('The column \"Borough\" has {} rows with the value \"Not Assigned\"'.\n",
    "     format(len(neighborhood[neighborhood['Borough']=='Not assigned'])))\n",
    "\n",
    "# Group our data by Postcode and Borough\n",
    "neighborhood = neighborhood.groupby(['Postcode', 'Borough'])['Neighbourhood'].apply(', '.join).reset_index()\n",
    "# Getting the .csv file from the url provided in the lab\n",
    "geo_coord_url = 'https://cocl.us/Geospatial_data'\n",
    "geo_coord_data = pd.read_csv(geo_coord_url)\n",
    "\n",
    "# A bit of manipulation for easier data merge\n",
    "geo_coord_data.columns = ['Postcode', 'Latitude', 'Longitude']\n",
    "# Merge of the two tables into a new one and see our result\n",
    "Toronto_geodata = pd.merge(neighborhood, geo_coord_data, how = 'left', on = 'Postcode')\n",
    "print(\"Our combined dataframe's shape is {}\".format(Toronto_geodata.shape))\n",
    "Toronto_geodata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foursquare api's credentials and initial values\n",
    "CLIENT_ID = 'CPTE1KSLDYYKCH4OJIF1FCD1ACUZNQD03KSM2ZKMPNVDDX1V' # your Foursquare ID\n",
    "CLIENT_SECRET = 'I0LYWHAVD2K1YW2ZRDLTC10DMEV141UGF32HM5QTEXG0GKWG' # your Foursquare Secret\n",
    "VERSION = '20180605' # Foursquare API version\n",
    "LIMIT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_near_by_venues function statement\n",
    "\n",
    "def get_near_by_venues(names, latitudes, longitudes, radius= 1000):    \n",
    "    venues_list=[]\n",
    "    \n",
    "    for name, lat, lng in zip(names, latitudes, longitudes):\n",
    "\n",
    "        # create the API request URL\n",
    "        url = 'https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}'\\\n",
    "        .format(CLIENT_ID, CLIENT_SECRET, VERSION, lat, lng, radius, LIMIT)    \n",
    "\n",
    "        # make the GET request\n",
    "        results = requests.get(url).json()[\"response\"]['groups'][0]['items']\n",
    "        # return only relevant information for each nearby venue\n",
    "        venues_list.append([(name, lat, lng, \n",
    "                             v['venue']['name'], v['venue']['location']['lat'], v['venue']['location']['lng'],\n",
    "                             v['venue']['categories'][0]['name']) for v in results])\n",
    "\n",
    "    nearby_venues = pd.DataFrame([item for venue in venues_list for item in venue])\n",
    "    nearby_venues.columns = ['Neighbourhood','Neighbourhood Latitude', 'Neighbourhood Longitude', \n",
    "                             'Venue', 'Venue Latitude', 'Venue Longitude', 'Venue Category']\n",
    "    \n",
    "    return nearby_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore_borough function statement\n",
    "\n",
    "def explore_borough(df, borough):\n",
    "    new_df = df[df['Borough'] == borough].reset_index(drop = True)\n",
    "    venues =  get_near_by_venues(names = new_df['Neighbourhood'],latitudes = new_df['Latitude'],\n",
    "                                 longitudes = new_df['Longitude'])\n",
    "\n",
    "    onehot_df = pd.get_dummies(venues[['Venue Category']], prefix= \"\", prefix_sep= \"\")\n",
    "\n",
    "    # # add neighborhood column back to dataframe\n",
    "    onehot_df['Neighbourhood'] = venues['Neighbourhood']\n",
    "    # move neighborhood column to the first column\n",
    "    fixed_columns = [onehot_df.columns[-1]] + list(onehot_df.columns[:-1])\n",
    "    onehot_df = onehot_df[fixed_columns]\n",
    "    onehot_df_grouped = onehot_df.groupby('Neighbourhood').mean().reset_index()\n",
    "    \n",
    "    onehot_coded_df = pd.merge(new_df, onehot_df_grouped, on = 'Neighbourhood', how = 'left')\n",
    "    \n",
    "    return onehot_coded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Venues4Boroughs function statement - The Final Piece\n",
    "\n",
    "def return_venues_for_boroughs(df):\n",
    "    col = list(df.columns)\n",
    "    all_venues = pd.DataFrame()\n",
    "    for i in df['Borough'].unique():\n",
    "        new_df = df[df['Borough'] == i]\n",
    "        print('For borough: ',i)\n",
    "        a = explore_borough(new_df, i)\n",
    "        all_venues = pd.concat([a, all_venues], axis = 0, ignore_index = True, sort = True)\n",
    "    cols = col + [j for j in all_venues.columns if j not in col]\n",
    "    all_venues = all_venues[cols]\n",
    "    return all_venues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for New York\n",
    "\n",
    "df1 = return_venues_for_boroughs(ny_df)\n",
    "df1.fillna(0,  inplace = True)\n",
    "print('Data Shape is: ', df1.shape)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for Toronto\n",
    "\n",
    "df2 = return_venues_for_boroughs(Toronto_geodata)\n",
    "df2.fillna(0, inplace = True)\n",
    "print('Data Shape is: ', df2.shape)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making deep copies of our dataframes, will be used later on\n",
    "ny_df = df1.copy()\n",
    "to_df = df2.copy().drop('Postcode', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After extensive trial & fail, we found out that there were inconsistencies in the NYC data. In more detail, there\n",
    "# were Neighbourhoods that have the same Name but are in different Boroughs. In order to fix this, we'll add \n",
    "# the Borough suffix for those Neighbourhoods.\n",
    "\n",
    "for i in range(ny_df.shape[0]):\n",
    "    nyn_ = ny_df.loc[i, 'Neighbourhood']\n",
    "    if ny_df[ny_df['Neighbourhood'] == nyn_].shape[0] > 1:\n",
    "        ind_ = ny_df[ny_df['Neighbourhood'] == nyn_].index.tolist()\n",
    "        for j in ind_:\n",
    "            nyb__ = ny_df.loc[j, 'Borough']\n",
    "            ny_df.loc[j, 'Neighbourhood'] = nyn_ + ', ' + nyb__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The code cells that follow is a test case, using Cosine similarity to find out the Nth most similar Neighbourhoods between a Neighbourhood in the other city.\n",
    "## This may not be the optimal way to go, and this is why we are using this as a test case, to compare between this method and our final one, using K Means Clustering\n",
    "#### This part may be easily skipped, as it was mainly used because I had the code below ready from another project and thought it worthwhile to take a look at this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most common venues between the two cities\n",
    "print('Leaving {} columns of New York city data\\nLeaving {} columns of Toronto city data'\n",
    "      .format(list(df1.columns[0:4]), list(df2.columns[0:5])))\n",
    "common = 0\n",
    "diff_in_NY = 0\n",
    "\n",
    "for i in df1.columns[4:]:\n",
    "    if i in df2.columns[5:]:\n",
    "        common += 1\n",
    "    else:\n",
    "        diff_in_NY += 1\n",
    "\n",
    "print('\\nNumber of common venue categories in both data are       : {}\\n\\\n",
    "Number of different venue categories in New York city are: {}\\n\\\n",
    "Number of different venue catehories in Toronto city are : {}'.format(common, diff_in_NY,\n",
    "                                                                     len(df2.columns[5:])-common))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vn1 = set(df1.columns[4:])\n",
    "vn2 = set(df2.columns[5:])\n",
    "plt.figure(figsize = (6,6))\n",
    "out = venn2([set(vn1), set(vn2)], set_labels = ['New York City', 'Toronto City'], set_colors=('purple', 'skyblue'), alpha = 0.5)\n",
    "for text in out.set_labels:\n",
    "    text.set_fontsize(18)\n",
    "for text in out.subset_labels:\n",
    "    text.set_fontsize(18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop non-common venues, for this we will be using the df sets\n",
    "\n",
    "comm_vns = []\n",
    "for i in df1.columns[4:]:\n",
    "    if i in df2.columns[5:]:\n",
    "        comm_vns.append(i)\n",
    "\n",
    "col1 = list(df1.columns[0:4]) + comm_vns\n",
    "col2 = list(df2.columns[0:5]) + comm_vns\n",
    "print('Before removing non-common venues, shape of New York: {}, and shape of Toronto is: {}'\n",
    "      .format(df1.shape, df2.shape))\n",
    "df1 = df1.loc[:, col1]\n",
    "df2 = df2.loc[:, col2]\n",
    "print('After removing non-common venues, shape of New York: {}, and shape of Toronto is: {}'\n",
    "      .format(df1.shape, df2.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions that uses cosine similarity to find N most similar places between NYC and TO, user defined statement\n",
    "\n",
    "# Function to add numeric suffix\n",
    "def ret_order_num(n_most_similar):\n",
    "    a = ['st', 'nd', 'rd']\n",
    "    c = []\n",
    "    for i in range(1,20+1):\n",
    "        if i == 1:\n",
    "            c.append(str(i)+str(a[i-1]))\n",
    "        elif i == 2:\n",
    "            c.append(str(i)+str(a[i-1]))\n",
    "        elif i == 3:\n",
    "            c.append(str(i)+str(a[i-1]))\n",
    "        else:\n",
    "            c.append(str(i)+'th')\n",
    "    return c\n",
    "\n",
    "def most_similar_borough(cur_city, cur_borough, cur_neigh, n_most_similar):\n",
    "    ny_data = df1.iloc[:,4:]\n",
    "    toronto_data = df2.iloc[:,5:]\n",
    "    if cur_city.lower() == 'New York'.lower():\n",
    "        X1 = ny_data.values\n",
    "        X2 = toronto_data.values\n",
    "        index = df1.loc[(df1['Borough'] == cur_borough) \n",
    "                        & (df1['Neighbourhood'] == cur_neigh)].index.values.astype(int)[0]\n",
    "        lat = df1.loc[index]['Latitude']\n",
    "        lon = df1.loc[index]['Longitude']\n",
    "    else:\n",
    "        X1 = toronto_data.values\n",
    "        X2 = ny_data.values\n",
    "        index = df2.loc[(df2['Borough'] == cur_borough) \n",
    "                & (df2['Neighbourhood'] == cur_neigh)].index.values.astype(int)[0]\n",
    "        lat = df2.loc[index]['Latitude']\n",
    "        lon = df2.loc[index]['Longitude']\n",
    "    \n",
    "    a = np.matmul(X1[index], X2.T)\n",
    "    aa = np.argsort(-a)[0:n_most_similar]\n",
    "    if cur_city.lower() == 'New York'.lower():\n",
    "        my_brgh = df2.iloc[aa, :]\n",
    "    else:\n",
    "        my_brgh = df1.iloc[aa, :]\n",
    "            \n",
    "    c = ret_order_num(n_most_similar)\n",
    "    \n",
    "    current_location_popup = '{}, {}, {}'.format(cur_neigh, cur_borough, cur_city)\n",
    "    loclabl = folium.Popup(current_location_popup, parse_html=True)\n",
    "    \n",
    "    my_map = folium.Map(location = [lat, lon], zoom_start = 6)\n",
    "    folium.CircleMarker([lat, lon], color = 'red', radius = 5,\n",
    "                        popup = loclabl, fill_color = '#3186cc', fill_opacity = 1,\n",
    "                        fill = True, tooltip = 'current location').add_to(my_map)\n",
    "\n",
    "    # add markers to map\n",
    "    for lat, lng, label, priority in zip(my_brgh['Latitude'], my_brgh['Longitude'], \n",
    "                                         my_brgh['Neighbourhood'], c):\n",
    "        label = folium.Popup(label, parse_html=True)\n",
    "        folium.Marker([lat, lng], radius = 5, popup=label, color='blue', \n",
    "                      tooltip = priority, parse_html=False).add_to(my_map)  \n",
    "\n",
    "    print('Using Cosine Similarity, we found out that the 10 most similar Neighbourhood to:', Cos_Neigh,\",\", Cos_Borough, 'are the following, \\nin descending order:')\n",
    "\n",
    "    for i in my_brgh:\n",
    "        cos_top10 = my_brgh[['Borough', 'Neighbourhood']]\n",
    "    print(cos_top10)\n",
    "    return my_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Cos_Neigh = 'East Village'\n",
    "Cos_Borough = \"Manhattan\"\n",
    "Cos_City = 'New York'\n",
    "\n",
    "most_similar_borough(cur_city = Cos_City, cur_borough = Cos_Borough,\n",
    "                     cur_neigh = Cos_Neigh, n_most_similar = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Returning to the main project at hand, the code cells above refer to an alternate methodology, using Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we define a function in order to return the first N most common venues\n",
    "def return_most_common_venues(row, start_col, n_top_cat):\n",
    "    row_categories = row.iloc[start_col:]\n",
    "    row_categories_sorted = row_categories.sort_values(ascending=False)\n",
    "    \n",
    "    return row_categories_sorted.index.values[0:n_top_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create columns according to number of top venues\n",
    "n_top_cat = 10\n",
    "indicators = ['st', 'nd', 'rd']\n",
    "columns = ['Neighbourhood']\n",
    "\n",
    "for ind in np.arange(n_top_cat):\n",
    "    try:\n",
    "        columns.append('{}{} Most Common Venue'.format(ind+1, indicators[ind]))\n",
    "    except:\n",
    "        columns.append('{}th Most Common Venue'.format(ind+1))\n",
    "\n",
    "# create a new dataframe\n",
    "nyc_top_cat = pd.DataFrame(columns=columns)\n",
    "nyc_top_cat['Neighbourhood'] = ny_df['Neighbourhood']\n",
    "\n",
    "for ind in np.arange(ny_df.shape[0]):\n",
    "    nyc_top_cat.iloc[ind, 1:] = return_most_common_venues(ny_df.iloc[ind, :], 3, n_top_cat)\n",
    "\n",
    "nyc_top_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering NYC\n",
    "Now we apply K-Means Clustering for our dataframe for NYC, which includes the relative frequency of each venue per neighbourhood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set number of clusters\n",
    "kclusters = 5\n",
    "\n",
    "nyc_clust = ny_df.drop(ny_df.iloc[:, 0:4], inplace = True, axis = 1)\n",
    "\n",
    "# run k-means clustering\n",
    "kmeans = KMeans(n_clusters=kclusters, random_state=0).fit(nyc_clust)\n",
    "\n",
    "# check cluster labels generated for each row in the dataframe\n",
    "kmeans.labels_[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add clustering labels\n",
    "# ny_df.insert(0, 'Cluster Labels', kmeans.labels_)\n",
    "\n",
    "nyc_merged = ny_df\n",
    "\n",
    "# merge toronto_grouped with toronto_data to add latitude/longitude for each neighborhood\n",
    "nyc_merged = nyc_merged.join(nyc_top_cat.set_index('Neighbourhood'), on='Neighbourhood')\n",
    "\n",
    "nyc_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ny_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
